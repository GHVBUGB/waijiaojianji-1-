#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†APIè·¯ç”±
"""
import os
import uuid
import zipfile
import asyncio
from datetime import datetime
from pathlib import Path
from typing import List, Optional
from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel

from app.api.routes.video import ApiResponse
from app.services.video_processor import video_processor
from app.config.settings import settings
from app.utils.logger import logger

router = APIRouter(prefix="/api/v1/batch", tags=["æ‰¹é‡å¤„ç†"])

# æ‰¹é‡ä»»åŠ¡çŠ¶æ€å­˜å‚¨
batch_jobs = {}

class BatchJobStatus(BaseModel):
    batch_id: str
    total_files: int
    completed_files: int
    failed_files: int
    status: str  # "processing", "completed", "failed"
    created_at: str
    completed_at: Optional[str] = None
    download_ready: bool = False
    download_path: Optional[str] = None
    job_ids: List[str] = []
    background_file_path: Optional[str] = None
    background_url: Optional[str] = None

class BatchProcessRequest(BaseModel):
    teacher_name: str
    language_hint: Optional[str] = None
    quality: str = "medium"
    output_format: str = "mp4"

@router.post("/upload", response_model=ApiResponse)
async def batch_upload_videos(
    files: List[UploadFile] = File(...),
    teacher_name: str = "Teacher",
    language_hint: Optional[str] = None,
    quality: str = "medium",
    output_format: str = "mp4",
    background_image: Optional[UploadFile] = File(None),
    background_url: Optional[str] = None,
    background_tasks: BackgroundTasks = None,
):
    """
    æ‰¹é‡ä¸Šä¼ è§†é¢‘æ–‡ä»¶
    """
    try:
        batch_id = str(uuid.uuid4())
        upload_dir = Path("uploads") / batch_id
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        uploaded_files = []
        background_file_path = None
        
        # å¤„ç†èƒŒæ™¯å›¾ç‰‡
        if background_image and background_image.filename:
            background_file_path = upload_dir / f"background_{background_image.filename}"
            with open(background_file_path, "wb") as f:
                content = await background_image.read()
                f.write(content)
            logger.info(f"èƒŒæ™¯å›¾ç‰‡å·²ä¿å­˜: {background_file_path}")
        
        for file in files:
            if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
                continue
                
            # ä¿å­˜æ–‡ä»¶
            file_path = upload_dir / file.filename
            with open(file_path, "wb") as f:
                content = await file.read()
                f.write(content)
            
            uploaded_files.append({
                "filename": file.filename,
                "path": str(file_path),
                "size": len(content)
            })
        
        if not uploaded_files:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘æ–‡ä»¶")
        
        # åˆå§‹åŒ–æ‰¹é‡ä»»åŠ¡çŠ¶æ€
        batch_jobs[batch_id] = BatchJobStatus(
            batch_id=batch_id,
            total_files=len(uploaded_files),
            completed_files=0,
            failed_files=0,
            status="uploaded",
            created_at=datetime.now().isoformat(),
            job_ids=[]
        )
        
        # ä¿å­˜èƒŒæ™¯ä¿¡æ¯åˆ°æ‰¹é‡ä»»åŠ¡ä¸­
        if background_file_path:
            batch_jobs[batch_id].background_file_path = str(background_file_path)
        elif background_url:
            batch_jobs[batch_id].background_url = background_url
        
        logger.info(f"æ‰¹é‡ä¸Šä¼ å®Œæˆ: {batch_id}, æ–‡ä»¶æ•°é‡: {len(uploaded_files)}")

        # è‡ªåŠ¨å¯åŠ¨æ‰¹é‡å¤„ç†ï¼ˆé¡ºåºå¤„ç†ï¼‰
        try:
            batch_jobs[batch_id].status = "processing"
            if background_tasks is not None:
                background_tasks.add_task(
                    process_batch_videos,
                    batch_id,
                    teacher_name,
                    language_hint,
                    quality,
                    output_format,
                )
                logger.info(f"å·²è‡ªåŠ¨å¯åŠ¨æ‰¹é‡å¤„ç†: {batch_id}")
        except Exception as e:
            logger.error(f"è‡ªåŠ¨å¯åŠ¨æ‰¹é‡å¤„ç†å¤±è´¥: {e}")

        return ApiResponse(
            success=True,
            message=f"æ‰¹é‡ä¸Šä¼ æˆåŠŸï¼Œå·²è‡ªåŠ¨å¼€å§‹å¤„ç†ï¼ˆå…±{len(uploaded_files)}ä¸ªæ–‡ä»¶ï¼‰",
            data={
                "batch_id": batch_id,
                "uploaded_files": uploaded_files,
                "total_files": len(uploaded_files)
            }
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/process/{batch_id}", response_model=ApiResponse)
async def start_batch_processing(
    batch_id: str,
    background_tasks: BackgroundTasks,
    request: BatchProcessRequest
):
    """
    å¼€å§‹æ‰¹é‡å¤„ç†è§†é¢‘
    """
    try:
        if batch_id not in batch_jobs:
            raise HTTPException(status_code=404, detail="æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨")
        
        batch_job = batch_jobs[batch_id]
        if batch_job.status != "uploaded":
            raise HTTPException(status_code=400, detail="æ‰¹é‡ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®")
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        batch_job.status = "processing"
        
        # åœ¨åå°å¯åŠ¨æ‰¹é‡å¤„ç†
        background_tasks.add_task(
            process_batch_videos,
            batch_id,
            request.teacher_name,
            request.language_hint,
            request.quality,
            request.output_format
        )
        
        return ApiResponse(
            success=True,
            message="æ‰¹é‡å¤„ç†å·²å¼€å§‹",
            data={"batch_id": batch_id, "status": "processing"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯åŠ¨æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_batch_videos(
    batch_id: str,
    teacher_name: str,
    language_hint: Optional[str],
    quality: str,
    output_format: str
):
    """
    åå°æ‰¹é‡å¤„ç†è§†é¢‘
    """
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†ä»»åŠ¡: {batch_id}")
    
    try:
        if batch_id not in batch_jobs:
            logger.error(f"æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨: {batch_id}")
            return
            
        batch_job = batch_jobs[batch_id]
        upload_dir = Path("uploads") / batch_id
        
        if not upload_dir.exists():
            logger.error(f"ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨: {upload_dir}")
            batch_job.status = "failed"
            return
        
        # è·å–æ‰€æœ‰è§†é¢‘æ–‡ä»¶
        video_files = list(upload_dir.glob("*.mp4")) + \
                     list(upload_dir.glob("*.avi")) + \
                     list(upload_dir.glob("*.mov")) + \
                     list(upload_dir.glob("*.mkv"))
        
        job_ids = []
        
        # è·å–èƒŒæ™¯ä¿¡æ¯
        background_file_path = getattr(batch_job, 'background_file_path', None)
        background_url = getattr(batch_job, 'background_url', None)
        
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # é€ä¸ªå¤„ç†è§†é¢‘
        for i, video_file in enumerate(video_files, 1):
            try:
                job_id = str(uuid.uuid4())
                job_ids.append(job_id)
                
                logger.info(f"å¼€å§‹å¤„ç†ç¬¬ {i}/{len(video_files)} ä¸ªè§†é¢‘: {video_file.name}")
                
                # å¯åŠ¨å•ä¸ªè§†é¢‘å¤„ç† - ä½¿ç”¨å®Œæ•´çš„å¤„ç†æµç¨‹
                await video_processor.process_teacher_video_background(
                    job_id=job_id,
                    video_path=str(video_file),
                    teacher_name=teacher_name,
                    original_filename=video_file.name,
                    language_hint=language_hint,
                    quality=quality,
                    output_format=output_format,
                    background_file_path=background_file_path
                )
                
                # ç­‰å¾…å¤„ç†å®Œæˆ
                max_wait_time = 600  # æœ€å¤§ç­‰å¾…10åˆ†é’Ÿ
                wait_time = 0
                
                while wait_time < max_wait_time:
                    progress = video_processor.get_job_progress(job_id)
                    if not progress:
                        logger.warning(f"ä»»åŠ¡ {job_id} è¿›åº¦ä¿¡æ¯ä¸¢å¤±")
                        batch_job.failed_files += 1
                        break
                    
                    status = progress.get("status")
                    current_progress = progress.get("progress", 0)
                    current_step = progress.get("current_step", "å¤„ç†ä¸­")
                    
                    logger.info(f"è§†é¢‘ {i}/{len(video_files)} è¿›åº¦: {current_progress}% - {current_step}")
                    
                    if status == "completed":
                        batch_job.completed_files += 1
                        logger.info(f"è§†é¢‘ {i}/{len(video_files)} å¤„ç†å®Œæˆ")
                        break
                    elif status == "failed":
                        batch_job.failed_files += 1
                        error_msg = progress.get("error", "æœªçŸ¥é”™è¯¯")
                        logger.error(f"è§†é¢‘ {i}/{len(video_files)} å¤„ç†å¤±è´¥: {error_msg}")
                        break
                    
                    await asyncio.sleep(3)  # æ¯3ç§’æ£€æŸ¥ä¸€æ¬¡
                    wait_time += 3
                
            except Exception as e:
                logger.error(f"å¤„ç†è§†é¢‘å¤±è´¥ {video_file.name}: {str(e)}")
                batch_job.failed_files += 1
        
        # æ›´æ–°æ‰¹é‡ä»»åŠ¡çŠ¶æ€
        batch_job.job_ids = job_ids
        batch_job.status = "completed" if batch_job.failed_files == 0 else "partial"
        batch_job.completed_at = datetime.now().isoformat()
        
        # åˆ›å»ºæ‰“åŒ…ä¸‹è½½
        if batch_job.completed_files > 0:
            download_path = await create_batch_download_package(batch_id, job_ids)
            batch_job.download_ready = True
            batch_job.download_path = download_path
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {batch_id}, æˆåŠŸ: {batch_job.completed_files}, å¤±è´¥: {batch_job.failed_files}")
        
    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {str(e)}")
        batch_jobs[batch_id].status = "failed"

async def create_batch_download_package(batch_id: str, job_ids: List[str]) -> str:
    """
    åˆ›å»ºæ‰¹é‡ä¸‹è½½æ‰“åŒ…æ–‡ä»¶
    """
    try:
        # åˆ›å»ºä¸‹è½½ç›®å½•
        download_dir = Path("outputs") / "batch_downloads"
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        package_name = f"batch_{batch_id[:8]}_{timestamp}"
        zip_path = download_dir / f"{package_name}.zip"
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for job_id in job_ids:
                progress = video_processor.get_job_progress(job_id)
                if not progress or progress.get("status") != "completed":
                    continue
                
                result = progress.get("result", {})
                
                # æ·»åŠ å¤„ç†åçš„è§†é¢‘
                processed_video = result.get("processed_video")
                if processed_video and os.path.exists(processed_video):
                    video_name = Path(processed_video).name
                    zipf.write(processed_video, f"videos/{video_name}")
                
                # æ·»åŠ å­—å¹•æ–‡ä»¶
                subtitle_file = result.get("subtitle_file")
                if subtitle_file and os.path.exists(subtitle_file):
                    srt_name = Path(subtitle_file).name
                    zipf.write(subtitle_file, f"subtitles/{srt_name}")
        
        logger.info(f"æ‰¹é‡ä¸‹è½½åŒ…åˆ›å»ºå®Œæˆ: {zip_path}")
        return str(zip_path)
        
    except Exception as e:
        logger.error(f"åˆ›å»ºä¸‹è½½åŒ…å¤±è´¥: {str(e)}")
        raise e

@router.get("/status/{batch_id}", response_model=ApiResponse)
async def get_batch_status(batch_id: str):
    """
    è·å–æ‰¹é‡å¤„ç†çŠ¶æ€
    """
    try:
        if batch_id not in batch_jobs:
            raise HTTPException(status_code=404, detail="æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨")
        
        batch_job = batch_jobs[batch_id]
        
        return ApiResponse(
            success=True,
            message="è·å–çŠ¶æ€æˆåŠŸ",
            data=batch_job.dict()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ‰¹é‡çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/download/{batch_id}")
async def download_batch_results(batch_id: str):
    """
    ä¸‹è½½æ‰¹é‡å¤„ç†ç»“æœ
    """
    try:
        if batch_id not in batch_jobs:
            raise HTTPException(status_code=404, detail="æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨")
        
        batch_job = batch_jobs[batch_id]
        
        if not batch_job.download_ready or not batch_job.download_path:
            raise HTTPException(status_code=400, detail="ä¸‹è½½åŒ…æœªå‡†å¤‡å¥½")
        
        if not os.path.exists(batch_job.download_path):
            raise HTTPException(status_code=404, detail="ä¸‹è½½æ–‡ä»¶ä¸å­˜åœ¨")
        
        filename = Path(batch_job.download_path).name
        
        return FileResponse(
            batch_job.download_path,
            media_type="application/zip",
            filename=filename
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸‹è½½æ‰¹é‡ç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/list", response_model=ApiResponse)
async def list_batch_jobs():
    """
    è·å–æ‰€æœ‰æ‰¹é‡ä»»åŠ¡åˆ—è¡¨
    """
    try:
        jobs_list = []
        for batch_id, job in batch_jobs.items():
            jobs_list.append({
                "batch_id": batch_id,
                "total_files": job.total_files,
                "completed_files": job.completed_files,
                "failed_files": job.failed_files,
                "status": job.status,
                "created_at": job.created_at,
                "completed_at": job.completed_at,
                "download_ready": job.download_ready
            })
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        jobs_list.sort(key=lambda x: x["created_at"], reverse=True)
        
        return ApiResponse(
            success=True,
            message="è·å–æ‰¹é‡ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
            data={"jobs": jobs_list, "total": len(jobs_list)}
        )
        
    except Exception as e:
        logger.error(f"è·å–æ‰¹é‡ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/{batch_id}", response_model=ApiResponse)
async def delete_batch_job(batch_id: str):
    """
    åˆ é™¤æ‰¹é‡ä»»åŠ¡åŠç›¸å…³æ–‡ä»¶
    """
    try:
        if batch_id not in batch_jobs:
            raise HTTPException(status_code=404, detail="æ‰¹é‡ä»»åŠ¡ä¸å­˜åœ¨")
        
        batch_job = batch_jobs[batch_id]
        
        # åˆ é™¤ä¸Šä¼ æ–‡ä»¶å¤¹
        upload_dir = Path("uploads") / batch_id
        if upload_dir.exists():
            import shutil
            shutil.rmtree(upload_dir)
        
        # åˆ é™¤ä¸‹è½½åŒ…
        if batch_job.download_path and os.path.exists(batch_job.download_path):
            os.remove(batch_job.download_path)
        
        # ä»å†…å­˜ä¸­åˆ é™¤ä»»åŠ¡
        del batch_jobs[batch_id]
        
        logger.info(f"æ‰¹é‡ä»»åŠ¡å·²åˆ é™¤: {batch_id}")
        
        return ApiResponse(
            success=True,
            message="æ‰¹é‡ä»»åŠ¡åˆ é™¤æˆåŠŸ",
            data={"batch_id": batch_id}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ‰¹é‡ä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
